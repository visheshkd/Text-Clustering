# -*- coding: utf-8 -*-
"""text_clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PFVkxBvYl8yvpxMpJviJeMHwnPM2XTfr
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
import copy as cp

#UPLOADING FILE
#Uploading the AMAZON-dataset file containing reviews and labels.
Amazon_Dataset = pd.read_csv('Amazon_Reviews.csv')
#getting the reviews column and converting values into list
Amazon_Dataset = Amazon_Dataset['Review']
amazon_review=Amazon_Dataset.values.tolist()
#creating a tf-idf matrix using panda dataframe method and tf-idf vectorizer
tf_idf = TfidfVectorizer()
A = tf_idf.fit_transform(amazon_review)#fit_transform() is used for the initial fitting of parameters on the training set x, but it also returns a transformed x′
data_frame = pd.DataFrame(A.toarray(),columns=tf_idf.get_feature_names())
#converting the panda dataframe into numpy-array and converting numpy array to grayscale image
print("1st question Output : Grayscale Image :\n")
numpy_arr = data_frame.to_numpy();
plot_show = plt.imshow(numpy_arr)
plot_show.set_cmap("gray")
plt.title("Grayscale image")
plt.xlabel("Reviews")
plt.ylabel("Words")
plt.show()


#PART 1 COMPLETE

#PART 2 START

#taking the five  positive words and five negative words
pos_5words = "good,like,love,well,worth" 
neg_5words = "no,only,other,old,some"
#Listing the words I selected
pos_words = pos_5words.split(",")
neg_words = neg_5words.split(",")
#concatenating both the Lists - pos_words and neg_words to create a bag_of_words
bag_of_words = pos_words + neg_words

#For representing each review in a vector space of these ten words (i.e., count matrix) as well as tf-idf weight matrix: -


#creating a count matrix of bag_of_words using panda dataframe method and Count vectorizer
count_v = CountVectorizer()
A2 = count_v.fit_transform(amazon_review)
data_frame2 = pd.DataFrame(A2.toarray(),columns=count_v.get_feature_names())
print("Count Matrix : \n")
print(data_frame2[bag_of_words])

#creating a tf-idf matrix of bag_of_words using panda dataframe method and tf-idf vectorizer
tf_idf2 = TfidfVectorizer()
A3 = tf_idf2.fit_transform(amazon_review)
data_frame3 = pd.DataFrame(A3.toarray(),columns=tf_idf2.get_feature_names())
print("Tf-idf Matrix : \n")
print(data_frame3[bag_of_words])

#PART2 COMPLETE

#PART 3 START

#Finding the positive and negative words from the dataframe2 and counting them separately.
#For each review, summing up the frequency of “positive” words and “negative” words

#sum of positive words
data_frame_pos=data_frame3[pos_words]
data_frame_pos['positive_words']=data_frame_pos.sum(axis=1)
data_frame_pos=data_frame_pos.drop(pos_words,1)
#sum of negative words
data_frame_neg=data_frame3[neg_words]
data_frame_neg['negative_words']=data_frame_neg.sum(axis=1)
data_frame_neg=data_frame_neg.drop(neg_words,1)
data_tfidf = pd.concat([data_frame_pos, data_frame_neg], axis=1)#concatenate side by side
print("Tf-Idf Matrix of positive and negative words : \n")
print(data_tfidf)

#KMEANS IMPLEMENTATION
X = data_tfidf.values.tolist()
X = np.array(X)
#print(X.shape)-(1000,2)
n=X.shape[0]#No. of objects , (p=X.shape[1])-No. of attributes
k= int(input('Number of clusters : ')) #k clusters
c=[] # intial centers
for i in range(k):
  x,y=input('Intial Centers :').split()
  c.append([x,y])
for i in range(len(c)):
  for j in range(len(c[0])):
    c[i][j]=float(c[i][j])
c=np.array(c)
c=c.T
no_of_iterations = 10000
def mykmeans(X, k, c):
  iterations_take = 0 #How many iterations did it take to found centers for each cluster
  #create an array for distances wrt to number of objects n
  for i in range(no_of_iterations):
    distances=np.array([]).reshape(n,0)
    for j in range(k):
      l2_norm=np.sqrt(np.sum((X-c[:,j])**2,axis=1)) #Euclidean distance between the centroid/center and the object data point
      distances=np.c_[distances, l2_norm]
    C=np.argmin(distances,axis=1)+1 # finding the minimum distance between centroid and object data point so that we can create clusters accordingly.(finding nearest centroid)
    previous_center=np.zeros(c.shape)
    previous_center=cp.deepcopy(c)
    Y={} #storing the results i.e clusters and data-points assigned to that respective clusters. 
    for i in range(k):
      Y[i+1]=np.array([]).reshape(2,0)
    for j in range(n):
      Y[C[j]]=np.c_[Y[C[j]],X[j]] #cluster_no. - data points(x1,x2....xn). Assign the point xi to the respective cluster j.
    for x in range(k):
      Y[x+1]=Y[x+1].T #transpose
    for i in range(k):
      c[:,i]=np.mean(Y[i+1],axis=0)#mean of all points xi assigned to the cluster j gives new centroid(updated center).
    iterations_take+=1
    if(np.linalg.norm((previous_center - c),axis=None))<=0.001:
      break

    #Graph-Plot for visualization
  plt.scatter(X[:,0],X[:,1],c='black',label="data points")
  plt.xlabel('X')
  plt.ylabel('Y')
  plt.title("Data points : Before Clustering")
  plt.legend()
  plt.show()
  colors=['Blue', 'red', 'green', 'magenta',]
  labels=['Cluster-1', 'Cluster-2', 'Cluster-3', 'Cluster-4']
  for i in range(k):
    plt.scatter(Y[i+1][:,0], Y[i+1][:,1], c=colors[i], label=labels[i])
  plt.scatter(c[0,:], c[1,:], s=150, c='yellow', label='centroids')
  plt.xlabel('X')
  plt.ylabel('Y')
  plt.title("Kmeans Clustering")
  plt.legend()
  plt.show()
  print('Iterations it took :',iterations_take)
  return Y
cluster=mykmeans(X,k,c)